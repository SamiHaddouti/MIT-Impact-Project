{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team MSM: Mini-Hackathon for synthesis prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CandidateEmbedding():\n",
    "\n",
    "    def __init__(self, embedding_file: str = \"data/formulas_to_embedding.pkl\") -> None:\n",
    "        with open(embedding_file, \"rb\") as f:\n",
    "            self.formulas_to_embedding = pickle.load(f)\n",
    "    \n",
    "    def get_embedding(self, target_formula: str) -> np.array:\n",
    "        return np.array(self.formulas_to_embedding[target_formula])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, csv_file: str = \"data/ground_truth_sets.csv\") -> None:\n",
    "\n",
    "        def convert_to_list_of_lists(cell):\n",
    "            return ast.literal_eval(cell)\n",
    "        \n",
    "        self.data = pd.read_csv(csv_file, converters={1: convert_to_list_of_lists})\n",
    "\n",
    "        self.formulas_to_embedding = CandidateEmbedding().formulas_to_embedding\n",
    "\n",
    "        with open(\"data/candidates.json\", \"r\") as f:\n",
    "            self.candidates = json.load(f)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[np.ndarray, List[int]]:\n",
    "        target_formula = self.data.iloc[idx, 0]\n",
    "        target_formula = self.formulas_to_embedding[target_formula]\n",
    "        precursor_indexes = []\n",
    "        precursor_formulas = self.data.iloc[idx, 1]\n",
    "        for p in precursor_formulas:\n",
    "             precursor_indexes.append(self.precursor_to_index(p))\n",
    "        return target_formula, precursor_indexes\n",
    "\n",
    "    def precursor_to_index(self, precursor_set: List[str]) -> int:\n",
    "        try:\n",
    "            index = self.candidates.index(precursor_set)\n",
    "        except ValueError:\n",
    "            index = -1  # Return -1 if the precursor_set is not found\n",
    "        return index\n",
    "\n",
    "def train_collate_fn(batch):\n",
    "    target_formulas, precursor_indexes = zip(*batch)\n",
    "    \n",
    "    # Convert target formulas to tensors\n",
    "    target_formulas = [torch.tensor(tf, dtype=torch.float32) for tf in target_formulas]\n",
    "    \n",
    "    # Pad precursor indexes to the same length\n",
    "    max_length = max(len(pf) for pf in precursor_indexes)\n",
    "    padded_precursor_indexes = [\n",
    "        torch.tensor(pf + [-1] * (max_length - len(pf)), dtype=torch.long) for pf in precursor_indexes\n",
    "    ]\n",
    "    \n",
    "    return torch.stack(target_formulas), torch.stack(padded_precursor_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "            def __init__(self, json_file: str = \"data/test_targets.json\") -> None:\n",
    "                with open(json_file, 'r') as f:\n",
    "                    self.data = json.load(f)\n",
    "                self.formulas_to_embedding = CandidateEmbedding().formulas_to_embedding\n",
    "            \n",
    "            def __len__(self) -> int:\n",
    "                return len(self.data)\n",
    "            \n",
    "            def __getitem__(self, idx: int) -> np.ndarray:\n",
    "                target_formula = self.data[idx]\n",
    "                target_formula = self.formulas_to_embedding[target_formula]\n",
    "                return target_formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynthesisPredictionModel(nn.Module):\n",
    "    def __init__(self, input_dim: int=512, hidden_dim: int=1024, output_dim: int=27106):\n",
    "        super(SynthesisPredictionModel, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden_layer = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, target_formula: np.ndarray) -> Tensor:\n",
    "        x = F.relu(self.input_layer(target_formula))\n",
    "        x = F.relu(self.hidden_layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of the last layer: Vector 27106 x 1\n",
    "\n",
    "Target in the ground truth: Sets of materials, have to be translated to [m, n, ...]\n",
    "\n",
    "Test target: Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRankLoss(nn.Module):\n",
    "    def __init__(self, margin: float=10.0):\n",
    "        \"\"\"\n",
    "        Custom loss to ensure the highest logits correspond to the correct indices.\n",
    "        Args:\n",
    "            margin (float): Minimum margin by which correct logits must exceed incorrect logits.\n",
    "        \"\"\"\n",
    "        super(CustomRankLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, logits, padded_correct_indices):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits (torch.Tensor): Model output of shape (batch_size, num_classes).\n",
    "            correct_indices (List[torch.Tensor]): List of tensors where each tensor contains the correct indices for each example in the batch.\n",
    "        Returns:\n",
    "            torch.Tensor: Computed loss.\n",
    "        \"\"\"\n",
    "        batch_size = logits.size(0)\n",
    "        loss = 0.0\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            correct_indices = padded_correct_indices[i]\n",
    "            valid_indices = correct_indices[correct_indices >= 0]\n",
    "            correct_logits = logits[i, valid_indices]  # Logits at correct indices\n",
    "\n",
    "            # Get the logits for all other indices (incorrect logits)\n",
    "            incorrect_logits = logits[i]\n",
    "            incorrect_logits = incorrect_logits[\n",
    "                torch.isin(\n",
    "                    elements=torch.arange(logits.size(1), device=logits.device), \n",
    "                    test_elements=valid_indices, \n",
    "                    invert=True,\n",
    "                    )\n",
    "                ]  # Remove correct indices\n",
    "            \n",
    "            # print(f\"Length of output vector: {len(logits[i])}\")\n",
    "            # print(f\"Length of correct indices: {len(correct_indices)}\")\n",
    "            # print(correct_indices)\n",
    "            # print(f\"Length of valid indices: {len(valid_indices)}\")\n",
    "            # print(valid_indices)\n",
    "            # print(f\"Length of correct logits: {len(correct_logits)}\")\n",
    "            # print(correct_logits)\n",
    "            # print(f\"Length of incorrect logits: {len(incorrect_logits)}\")\n",
    "            # print(\"\")\n",
    "            \n",
    "            # Margin-based ranking loss\n",
    "            # Make sure to unsqueeze dimensions for broadcasting (correct_logits: (num_correct, 1), incorrect_logits: (num_incorrect,))\n",
    "            pairwise_losses = torch.relu(self.margin + incorrect_logits.unsqueeze(0) - correct_logits.unsqueeze(1))\n",
    "            \n",
    "            # Mean pairwise loss for this example\n",
    "            loss += pairwise_losses.mean()\n",
    "\n",
    "        return loss / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_reciprocal_rank(predicted_indices, correct_indices):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Reciprocal Rank (MRR) for the batch.\n",
    "    \n",
    "    Args:\n",
    "        predicted_indices (torch.Tensor): Tensor of shape (batch_size, num_classes) with the predicted ranks.\n",
    "        correct_indices (torch.Tensor): Tensor of shape (batch_size, num_correct) with the correct indices.\n",
    "    \n",
    "    Returns:\n",
    "        float: Mean Reciprocal Rank for the batch.\n",
    "    \"\"\"\n",
    "    batch_size = predicted_indices.size(0)\n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # Get the rank of the first correct index in the sorted predictions\n",
    "        correct_index_set = set(correct_indices[i].tolist())\n",
    "        for rank, idx in enumerate(predicted_indices[i].tolist(), start=1):\n",
    "            if idx in correct_index_set:\n",
    "                reciprocal_ranks.append(1.0 / rank)\n",
    "                break\n",
    "        else:\n",
    "            reciprocal_ranks.append(0.0)  # No correct index found in the predictions\n",
    "    \n",
    "    return sum(reciprocal_ranks) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_mrr = 0.0\n",
    "    batch_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (target_formulas, padded_precursor_indexes) in enumerate(dataloader):\n",
    "            # Move data to the same device as the model\n",
    "            target_formulas = target_formulas.to(device)\n",
    "            padded_precursor_indexes = [indices.to(device) for indices in padded_precursor_indexes]\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(target_formulas)  # Shape: (batch_size, output_dim)\n",
    "\n",
    "            # Get the predicted indices sorted by logits (descending order)\n",
    "            _, predicted_indices = torch.sort(logits, descending=True)\n",
    "\n",
    "            # Calculate MRR\n",
    "            batch_mrr = mean_reciprocal_rank(predicted_indices, torch.stack(padded_precursor_indexes))\n",
    "            total_mrr += batch_mrr\n",
    "            batch_count += 1\n",
    "\n",
    "    avg_mrr = total_mrr / batch_count\n",
    "    print(f\"Evaluation Complete. Average MRR: {avg_mrr:.4f}\")\n",
    "    return avg_mrr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Complete. Average Loss: 87.2347\n",
      "Evaluation Complete. Average MRR: 0.0126\n",
      "Epoch [2/100] Complete. Average Loss: 19.4179\n",
      "Evaluation Complete. Average MRR: 0.0131\n",
      "Epoch [3/100] Complete. Average Loss: 14.8596\n",
      "Evaluation Complete. Average MRR: 0.0132\n",
      "Epoch [4/100] Complete. Average Loss: 14.2317\n",
      "Evaluation Complete. Average MRR: 0.0137\n",
      "Epoch [5/100] Complete. Average Loss: 13.8618\n",
      "Evaluation Complete. Average MRR: 0.0140\n",
      "Epoch [6/100] Complete. Average Loss: 13.5755\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the train set every 5 epochs\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m save_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     68\u001b[0m     checkpoint_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoints_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_epoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, dataloader, device)\u001b[0m\n\u001b[1;32m      4\u001b[0m batch_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_formulas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadded_precursor_indexes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Move data to the same device as the model\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_formulas\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_formulas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadded_precursor_indexes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpadded_precursor_indexes\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/mit-impact-project/lib/python3.12/site-packages/torch/utils/data/dataloader.py:697\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 697\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_profile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;49;00m\n\u001b[1;32m    700\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-arg]\u001b[39;49;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/mit-impact-project/lib/python3.12/site-packages/torch/autograd/profiler.py:749\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;66;03m# TODO: Too slow with __torch_function__ handling enabled\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# See https://github.com/pytorch/pytorch/issues/76410\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[0;32m--> 749\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDisableTorchFunctionSubclass\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    750\u001b[0m         torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_exit\u001b[38;5;241m.\u001b[39m_RecordFunction(record)\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "log_interval = 1\n",
    "save_interval = 10\n",
    "eval_interval = 1\n",
    "\n",
    "checkpoints_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "\n",
    "train_dataset = TrainDataset()\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=train_collate_fn)\n",
    "\n",
    "test_dataset = TestDataset()\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "model = SynthesisPredictionModel()\n",
    "\n",
    "# Define optimizer and custom loss function\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "criterion = CustomRankLoss(margin=100.0)\n",
    "\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    batch_count = 0\n",
    "\n",
    "    for batch_idx, (target_formulas, padded_precursor_indexes) in enumerate(train_dataloader):\n",
    "        # Move data to the same device as the model\n",
    "        target_formulas = target_formulas.to(device)\n",
    "        padded_precursor_indexes = [indices.to(device) for indices in padded_precursor_indexes]\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(target_formulas)  # Shape: (batch_size, output_dim)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, padded_precursor_indexes)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        batch_count += 1\n",
    "\n",
    "        # Log progress\n",
    "        # if (batch_idx + 1) % log_interval == 0:\n",
    "        #     print(\n",
    "        #         f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "        #         f\"Batch [{batch_idx + 1}/{len(train_dataloader)}], \"\n",
    "        #         f\"Loss: {loss.item():.4f}\"\n",
    "        #     )\n",
    "\n",
    "\n",
    "    avg_loss = total_loss / batch_count\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] Complete. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Evaluate the model on the train set every 5 epochs\n",
    "    if (epoch + 1) % eval_interval == 0:\n",
    "        evaluate_model(model, train_dataloader, device)\n",
    "\n",
    "    if (epoch + 1) % save_interval == 0:\n",
    "        checkpoint_path = os.path.join(checkpoints_dir, f\"model_epoch_{epoch + 1}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Model saved to {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Complete. Average Loss: 89.7008\n",
      "Evaluation Complete. Average MRR: 0.0075\n",
      "Epoch [2/100] Complete. Average Loss: 17.5271\n",
      "Evaluation Complete. Average MRR: 0.0087\n",
      "Epoch [3/100] Complete. Average Loss: 13.0364\n",
      "Evaluation Complete. Average MRR: 0.0092\n",
      "Epoch [4/100] Complete. Average Loss: 12.4751\n",
      "Evaluation Complete. Average MRR: 0.0101\n",
      "Epoch [5/100] Complete. Average Loss: 12.1896\n",
      "Evaluation Complete. Average MRR: 0.0103\n",
      "Epoch [6/100] Complete. Average Loss: 11.9419\n",
      "Evaluation Complete. Average MRR: 0.0110\n",
      "Epoch [7/100] Complete. Average Loss: 11.7364\n",
      "Evaluation Complete. Average MRR: 0.0114\n",
      "Epoch [8/100] Complete. Average Loss: 11.5087\n",
      "Evaluation Complete. Average MRR: 0.0118\n",
      "Epoch [9/100] Complete. Average Loss: 11.1749\n",
      "Evaluation Complete. Average MRR: 0.0156\n",
      "Epoch [10/100] Complete. Average Loss: 10.4544\n",
      "Evaluation Complete. Average MRR: 0.0197\n",
      "Model saved to checkpoints2/model_epoch_10.pth\n",
      "Epoch [11/100] Complete. Average Loss: 9.1984\n",
      "Evaluation Complete. Average MRR: 0.0241\n",
      "Epoch [12/100] Complete. Average Loss: 7.4675\n",
      "Evaluation Complete. Average MRR: 0.0263\n",
      "Epoch [13/100] Complete. Average Loss: 5.7344\n",
      "Evaluation Complete. Average MRR: 0.0305\n",
      "Epoch [14/100] Complete. Average Loss: 4.4151\n",
      "Evaluation Complete. Average MRR: 0.0334\n",
      "Epoch [15/100] Complete. Average Loss: 3.3497\n",
      "Evaluation Complete. Average MRR: 0.0361\n",
      "Epoch [16/100] Complete. Average Loss: 2.3943\n",
      "Evaluation Complete. Average MRR: 0.0420\n",
      "Epoch [17/100] Complete. Average Loss: 1.6599\n",
      "Evaluation Complete. Average MRR: 0.0474\n",
      "Epoch [18/100] Complete. Average Loss: 1.2032\n",
      "Evaluation Complete. Average MRR: 0.0515\n",
      "Epoch [19/100] Complete. Average Loss: 0.9230\n",
      "Evaluation Complete. Average MRR: 0.0516\n",
      "Epoch [20/100] Complete. Average Loss: 0.7503\n",
      "Evaluation Complete. Average MRR: 0.0508\n",
      "Model saved to checkpoints2/model_epoch_20.pth\n",
      "Epoch [21/100] Complete. Average Loss: 0.6337\n",
      "Evaluation Complete. Average MRR: 0.0555\n",
      "Epoch [22/100] Complete. Average Loss: 0.5476\n",
      "Evaluation Complete. Average MRR: 0.0535\n",
      "Epoch [23/100] Complete. Average Loss: 0.4832\n",
      "Evaluation Complete. Average MRR: 0.0568\n",
      "Epoch [24/100] Complete. Average Loss: 0.4314\n",
      "Evaluation Complete. Average MRR: 0.0597\n",
      "Epoch [25/100] Complete. Average Loss: 0.3895\n",
      "Evaluation Complete. Average MRR: 0.0563\n",
      "Epoch [26/100] Complete. Average Loss: 0.3533\n",
      "Evaluation Complete. Average MRR: 0.0559\n",
      "Epoch [27/100] Complete. Average Loss: 0.3222\n",
      "Evaluation Complete. Average MRR: 0.0584\n",
      "Epoch [28/100] Complete. Average Loss: 0.2960\n",
      "Evaluation Complete. Average MRR: 0.0592\n",
      "Epoch [29/100] Complete. Average Loss: 0.2716\n",
      "Evaluation Complete. Average MRR: 0.0612\n",
      "Epoch [30/100] Complete. Average Loss: 0.2516\n",
      "Evaluation Complete. Average MRR: 0.0647\n",
      "Model saved to checkpoints2/model_epoch_30.pth\n",
      "Epoch [31/100] Complete. Average Loss: 0.2324\n",
      "Evaluation Complete. Average MRR: 0.0648\n",
      "Epoch [32/100] Complete. Average Loss: 0.2152\n",
      "Evaluation Complete. Average MRR: 0.0630\n",
      "Epoch [33/100] Complete. Average Loss: 0.2007\n",
      "Evaluation Complete. Average MRR: 0.0661\n",
      "Epoch [34/100] Complete. Average Loss: 0.1875\n",
      "Evaluation Complete. Average MRR: 0.0697\n",
      "Epoch [35/100] Complete. Average Loss: 0.1749\n",
      "Evaluation Complete. Average MRR: 0.0647\n",
      "Epoch [36/100] Complete. Average Loss: 0.1636\n",
      "Evaluation Complete. Average MRR: 0.0687\n",
      "Epoch [37/100] Complete. Average Loss: 0.1536\n",
      "Evaluation Complete. Average MRR: 0.0730\n",
      "Epoch [38/100] Complete. Average Loss: 0.1449\n",
      "Evaluation Complete. Average MRR: 0.0716\n",
      "Epoch [39/100] Complete. Average Loss: 0.1357\n",
      "Evaluation Complete. Average MRR: 0.0718\n",
      "Epoch [40/100] Complete. Average Loss: 0.1295\n",
      "Evaluation Complete. Average MRR: 0.0767\n",
      "Model saved to checkpoints2/model_epoch_40.pth\n",
      "Epoch [41/100] Complete. Average Loss: 0.1222\n",
      "Evaluation Complete. Average MRR: 0.0729\n",
      "Epoch [42/100] Complete. Average Loss: 0.1159\n",
      "Evaluation Complete. Average MRR: 0.0759\n",
      "Epoch [43/100] Complete. Average Loss: 0.1125\n",
      "Evaluation Complete. Average MRR: 0.0781\n",
      "Epoch [44/100] Complete. Average Loss: 0.1045\n",
      "Evaluation Complete. Average MRR: 0.0793\n",
      "Epoch [45/100] Complete. Average Loss: 0.1007\n",
      "Evaluation Complete. Average MRR: 0.0822\n",
      "Epoch [46/100] Complete. Average Loss: 0.0970\n",
      "Evaluation Complete. Average MRR: 0.0760\n",
      "Epoch [47/100] Complete. Average Loss: 0.0922\n",
      "Evaluation Complete. Average MRR: 0.0792\n",
      "Epoch [48/100] Complete. Average Loss: 0.0883\n",
      "Evaluation Complete. Average MRR: 0.0803\n",
      "Epoch [49/100] Complete. Average Loss: 0.0866\n",
      "Evaluation Complete. Average MRR: 0.0828\n",
      "Epoch [50/100] Complete. Average Loss: 0.0818\n",
      "Evaluation Complete. Average MRR: 0.0819\n",
      "Model saved to checkpoints2/model_epoch_50.pth\n",
      "Epoch [51/100] Complete. Average Loss: 0.0796\n",
      "Evaluation Complete. Average MRR: 0.0823\n",
      "Epoch [52/100] Complete. Average Loss: 0.0761\n",
      "Evaluation Complete. Average MRR: 0.0877\n",
      "Epoch [53/100] Complete. Average Loss: 0.0742\n",
      "Evaluation Complete. Average MRR: 0.0764\n",
      "Epoch [54/100] Complete. Average Loss: 0.0717\n",
      "Evaluation Complete. Average MRR: 0.0881\n",
      "Epoch [55/100] Complete. Average Loss: 0.0676\n",
      "Evaluation Complete. Average MRR: 0.0887\n",
      "Epoch [56/100] Complete. Average Loss: 0.0683\n",
      "Evaluation Complete. Average MRR: 0.0811\n",
      "Epoch [57/100] Complete. Average Loss: 0.0653\n",
      "Evaluation Complete. Average MRR: 0.0890\n",
      "Epoch [58/100] Complete. Average Loss: 0.0614\n",
      "Evaluation Complete. Average MRR: 0.0925\n",
      "Epoch [59/100] Complete. Average Loss: 0.0591\n",
      "Evaluation Complete. Average MRR: 0.0910\n",
      "Epoch [60/100] Complete. Average Loss: 0.0574\n",
      "Evaluation Complete. Average MRR: 0.0973\n",
      "Model saved to checkpoints2/model_epoch_60.pth\n",
      "Epoch [61/100] Complete. Average Loss: 0.0568\n",
      "Evaluation Complete. Average MRR: 0.0831\n",
      "Epoch [62/100] Complete. Average Loss: 0.0552\n",
      "Evaluation Complete. Average MRR: 0.0926\n",
      "Epoch [63/100] Complete. Average Loss: 0.0515\n",
      "Evaluation Complete. Average MRR: 0.0921\n",
      "Epoch [64/100] Complete. Average Loss: 0.0502\n",
      "Evaluation Complete. Average MRR: 0.0908\n",
      "Epoch [65/100] Complete. Average Loss: 0.0489\n",
      "Evaluation Complete. Average MRR: 0.0935\n",
      "Epoch [66/100] Complete. Average Loss: 0.0469\n",
      "Evaluation Complete. Average MRR: 0.0950\n",
      "Epoch [67/100] Complete. Average Loss: 0.0454\n",
      "Evaluation Complete. Average MRR: 0.1004\n",
      "Epoch [68/100] Complete. Average Loss: 0.0440\n",
      "Evaluation Complete. Average MRR: 0.0983\n",
      "Epoch [69/100] Complete. Average Loss: 0.0421\n",
      "Evaluation Complete. Average MRR: 0.0984\n",
      "Epoch [70/100] Complete. Average Loss: 0.0408\n",
      "Evaluation Complete. Average MRR: 0.0921\n",
      "Model saved to checkpoints2/model_epoch_70.pth\n",
      "Epoch [71/100] Complete. Average Loss: 0.0393\n",
      "Evaluation Complete. Average MRR: 0.0951\n",
      "Epoch [72/100] Complete. Average Loss: 0.0376\n",
      "Evaluation Complete. Average MRR: 0.1046\n",
      "Epoch [73/100] Complete. Average Loss: 0.0394\n",
      "Evaluation Complete. Average MRR: 0.0981\n",
      "Epoch [74/100] Complete. Average Loss: 0.0357\n",
      "Evaluation Complete. Average MRR: 0.1000\n",
      "Epoch [75/100] Complete. Average Loss: 0.0343\n",
      "Evaluation Complete. Average MRR: 0.0985\n",
      "Epoch [76/100] Complete. Average Loss: 0.0336\n",
      "Evaluation Complete. Average MRR: 0.1000\n",
      "Epoch [77/100] Complete. Average Loss: 0.0324\n",
      "Evaluation Complete. Average MRR: 0.0999\n",
      "Epoch [78/100] Complete. Average Loss: 0.0316\n",
      "Evaluation Complete. Average MRR: 0.1023\n",
      "Epoch [79/100] Complete. Average Loss: 0.0303\n",
      "Evaluation Complete. Average MRR: 0.1097\n",
      "Epoch [80/100] Complete. Average Loss: 0.0292\n",
      "Evaluation Complete. Average MRR: 0.1048\n",
      "Model saved to checkpoints2/model_epoch_80.pth\n",
      "Epoch [81/100] Complete. Average Loss: 0.0287\n",
      "Evaluation Complete. Average MRR: 0.1068\n",
      "Epoch [82/100] Complete. Average Loss: 0.0271\n",
      "Evaluation Complete. Average MRR: 0.1094\n",
      "Epoch [83/100] Complete. Average Loss: 0.0360\n",
      "Evaluation Complete. Average MRR: 0.1058\n",
      "Epoch [84/100] Complete. Average Loss: 0.0304\n",
      "Evaluation Complete. Average MRR: 0.1068\n",
      "Epoch [85/100] Complete. Average Loss: 0.0256\n",
      "Evaluation Complete. Average MRR: 0.1084\n",
      "Epoch [86/100] Complete. Average Loss: 0.0246\n",
      "Evaluation Complete. Average MRR: 0.1073\n",
      "Epoch [87/100] Complete. Average Loss: 0.0240\n",
      "Evaluation Complete. Average MRR: 0.1080\n",
      "Epoch [88/100] Complete. Average Loss: 0.0238\n",
      "Evaluation Complete. Average MRR: 0.1067\n",
      "Epoch [89/100] Complete. Average Loss: 0.0234\n",
      "Evaluation Complete. Average MRR: 0.1054\n",
      "Epoch [90/100] Complete. Average Loss: 0.0225\n",
      "Evaluation Complete. Average MRR: 0.1161\n",
      "Model saved to checkpoints2/model_epoch_90.pth\n",
      "Epoch [91/100] Complete. Average Loss: 0.0216\n",
      "Evaluation Complete. Average MRR: 0.1158\n",
      "Epoch [92/100] Complete. Average Loss: 0.0207\n",
      "Evaluation Complete. Average MRR: 0.1169\n",
      "Epoch [93/100] Complete. Average Loss: 0.0201\n",
      "Evaluation Complete. Average MRR: 0.1158\n",
      "Epoch [94/100] Complete. Average Loss: 0.0195\n",
      "Evaluation Complete. Average MRR: 0.1219\n",
      "Epoch [95/100] Complete. Average Loss: 0.0191\n",
      "Evaluation Complete. Average MRR: 0.1128\n",
      "Epoch [96/100] Complete. Average Loss: 0.0183\n",
      "Evaluation Complete. Average MRR: 0.1190\n",
      "Epoch [97/100] Complete. Average Loss: 0.0182\n",
      "Evaluation Complete. Average MRR: 0.1280\n",
      "Epoch [98/100] Complete. Average Loss: 0.0173\n",
      "Evaluation Complete. Average MRR: 0.1204\n",
      "Epoch [99/100] Complete. Average Loss: 0.0167\n",
      "Evaluation Complete. Average MRR: 0.1149\n",
      "Epoch [100/100] Complete. Average Loss: 0.0164\n",
      "Evaluation Complete. Average MRR: 0.1163\n",
      "Model saved to checkpoints2/model_epoch_100.pth\n"
     ]
    }
   ],
   "source": [
    "# Training loop parameters\n",
    "num_epochs = 100\n",
    "log_interval = 1\n",
    "save_interval = 10\n",
    "eval_interval = 1\n",
    "\n",
    "checkpoints_dir = \"checkpoints2\"\n",
    "os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "\n",
    "# Load dataset\n",
    "dataset = TrainDataset()\n",
    "\n",
    "# Split dataset into 80% train and 20% evaluate\n",
    "train_size = int(0.8 * len(dataset))\n",
    "eval_size = len(dataset) - train_size\n",
    "train_dataset, eval_dataset = random_split(dataset, [train_size, eval_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=train_collate_fn)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=4, shuffle=False, collate_fn=train_collate_fn)\n",
    "\n",
    "model = SynthesisPredictionModel()\n",
    "\n",
    "# Define optimizer and custom loss function\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "criterion = CustomRankLoss(margin=100.0)\n",
    "\n",
    "# Training Loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    batch_count = 0\n",
    "\n",
    "    for batch_idx, (target_formulas, padded_precursor_indexes) in enumerate(train_dataloader):\n",
    "        # Move data to the same device as the model\n",
    "        target_formulas = target_formulas.to(device)\n",
    "        padded_precursor_indexes = [indices.to(device) for indices in padded_precursor_indexes]\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(target_formulas)  # Shape: (batch_size, output_dim)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, padded_precursor_indexes)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        batch_count += 1\n",
    "\n",
    "    avg_loss = total_loss / batch_count\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] Complete. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Evaluate the model on the eval set every eval_interval epochs\n",
    "    if (epoch + 1) % eval_interval == 0:\n",
    "        evaluate_model(model, eval_dataloader, device)\n",
    "\n",
    "    # Save the model checkpoint every save_interval epochs\n",
    "    if (epoch + 1) % save_interval == 0:\n",
    "        checkpoint_path = os.path.join(checkpoints_dir, f\"model_epoch_{epoch + 1}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Model saved to {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
